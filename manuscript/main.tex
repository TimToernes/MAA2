%\documentclass[11pt,a4paper,english,twocolumn]{article}
\documentclass[11pt,a4paper,english]{article}
% Graphics packages 
\usepackage{graphicx}
% Include the Euro character
\usepackage[utf8]{inputenc}
\usepackage{lmodern,textcomp}

% Line numbering
%\usepackage[switch,columnwise]{lineno}
%\linenumbers

% Math Package
\usepackage{mathtools}
\usepackage{amsmath}
% Bibliography package 
\usepackage[backend=bibtex, style=numeric,citestyle=numeric,sorting=none, backref=none, backrefstyle=all+]{biblatex}
\addbibresource{references.bib}

\begin{document}
	
The goal of this method is to evenly sample the near-optimal feasible decision space of a linear optimization model using a version of Markov Chain Monte Carlo (MCMC) sampling named the hit and run algorithm \cite{chen_fast_nodate, kiatsupaibul_analysis_2011}.
	
\section{Model}

The model under consideration is linear and convex and can be written on the standard form: 

\begin{align}\label{eq:ConvexOptimization}
\text{minimize} \;&\; \mathbf{f}_0(\mathbf{x})  \\
	\text{subject to} \; &\; \mathbf{f}_i(\mathbf{x}) \leq 0 \; \; i=1...m\\
\;            &\;  \mathbf{h}_j(\mathbf{x}) = 0 \; \; j=1...p
\end{align}

With $\mathbf{x}\in \mathbf{R}^n$. Besides from the constraints contained in the original optimization problem, an additional constraint is introduced, to separate the near-optimal feasible space, from the entire feasible space. 
\begin{equation}\label{eq:MGA_const}
f_0(\mathbf{x}) \leq f_0(\mathbf{\hat{x}})\cdot (1+\epsilon)
\end{equation}

Where $\mathbf{\hat{x}}$ is an optimal solution to the original problem and $f_0(\mathbf{\hat{x}})$ is the objective value for the optimal solution. The slack on optimality is given by the slack variable $\theta$ .

As the constraints are linear they can be written as matrix products on the form: 

\begin{align}
\mathbf{f}_i(\mathbf{x}) \leq 0 \Rightarrow A  \mathbf{x}  \leq \mathbf{b} \\
\mathbf{h}_j(\mathbf{x}) = 0 \Rightarrow H  \mathbf{x} = \mathbf{c}
\end{align}

The set containing all feasible near-optimal solutions then become:
\begin{equation}
F = \{\mathbf{x} \in \mathbf{R}^n | A  \mathbf{x}  \leq \mathbf{b} \wedge H  \mathbf{x} = \mathbf{c} \}
\end{equation}


\section{Presolve}

In order to apply the hit and run sampling algorithm, a fully dimensional space is required. As several equalities are included in the definition of $F$ and hidden equalities might be found among the inequality constraints, the fully dimensional subspace of $F$ must be found. Hidden equalities occur when two inequalities limit the range of a variable to zero. 

The goal of the presolve process is to define the fully dimensional subspace $Z$ of $F$. 

\subsection{Step 1}
Initially all linearly dependent constraints/rows in the augmented matrix $[ A|\mathbf{b}] $ are identified. Linearly dependent constraints/rows, span parallel hyper-planes in $F$. If two such hyper-planes were to coincide, and have normal vectors pointing in opposite directions, they constrain a dimension of $F$ and can be represented as an equality rather than two inequalities. For all such inequality constraints constraining a dimension of $F$, their given row in $A$ are moved from the matrix $A$ to the $H$ matrix.

\subsection{Step 2}
From \cite{ConvexOpimization} (p. 523) we know that the problem can be reformulated as: 

\begin{align}
\text{minimize} \;&\; \mathbf{f}_0(\mathbf{\hat{x}} + N \mathbf{z})  \\
\text{subject to} \; &\; A(\mathbf{\hat{x}} + N \mathbf{z}) \leq \mathbf{b} \; \; i=1...m\\
\;            &\;  span(N) = \mathcal{N}(H)
\end{align}

Where the span of $N$ is the null space of $H$ and $\mathbf{\hat{x}}$ is any particular solution contained in $F$.
As all equalities have been eliminated from the problem, the fully dimensional subspace of $F$ can be defined as:

\begin{equation}
Z = \{\mathbf{z} \in \mathbf{R}^{n-p} | A(\mathbf{\hat{x}} +  N \mathbf{z}) <= \mathbf{b}  \}
\end{equation}

Using this the optimization problem can be reformulated as:

\begin{align}
	\text{minimize} \;&\; \mathbf{f}_0(\mathbf{\hat{x}} + N \mathbf{z})  \\
	\text{subject to} \; &\; \hat{A}\mathbf{z} \leq \mathbf{\hat{b}} \label{eq:sub_problem}
\end{align}

With $\hat{A}$ and $ \mathbf{\hat{b}}$:

\begin{align}
	\;            &\;  \hat{A} = A \cdot N \\ 
	\;			& \; \mathbf{\hat{b}} = \mathbf{b}- A\mathbf{\hat{x}}
\end{align}

By validating that the matrix $\hat{A}$ has full rank, we know that the subspace $Z$ is fully dimensional. 

\subsection{Calculating the Null space $N$}

Using QR decomposition the Null space of a matrix can be calculated. 

Given the $(m \times n)$matrix $H$, it can be decomposed to the orthogonal $(m\times m)$ matrix Q, and R which is a $(m \times n)$ upper triangular matrix. 

\begin{equation}
	H = QR = \left[Q_1  Q_2 \right] \left[{R_1 \atop 0} \right]
\end{equation}

Where $Q_1$ is $(m \times r )$ and $Q_2$ $(m\times m-r)$, with $r$ being the rank of $H$. 

The null space of $H$ is can then by found as $Q_2 = N(H^T)$

For proof see \cite{Hyde}
\section{Hit and run sampling}

A version of the hit and run algorithm \cite{Smith1984}, is used to sample the subspace $Z$. The algorithm consist of an iterative process of drawing random directions, and taking steps of random lengths within the space $Z$. 

Initially a point $\mathbf{z}_0$ inside the space $Z$ must be provided. Random steps inside Z are then taken by generating a random direction $\theta$ and taking a random step $t$ that will not violate the boundaries of $Z$. 

\begin{equation}\label{eq:step}
	\mathbf{z}_{i+1} = \mathbf{z}_i + \theta t
\end{equation}

%todo update this 
$\theta$ must be a unit vector pointing in a random direction evenly distributed on a unit hyper sphere $\mathbf{S}^{n-p}$. I will elaborate on how to do so!!! But this is easy.

To determine the range of $t$ that ensures that the step $t$ does not cross the boundary of $Z$, Equation \ref{eq:step} is substituded in to Equation \ref{eq:sub_problem}.

\begin{equation}
	\hat{A}(\mathbf{z}_i + \theta t) \leq \hat{b}
\end{equation}

Isolating $t$ will result in the upper and lower bounds:

\begin{align}
t_{max} =  \text{inf} \frac{\hat{b}-\hat{A}\mathbf{z}_i}{\hat{A}\theta}& \; \text{given }  \; \hat{A}\theta>0\\
t_{min} = \text{sup} \frac{\hat{b}-\hat{A}\mathbf{z}_i}{\hat{A}\theta}& \; \text{given }  \; \hat{A}\theta<0
\end{align}

Selecting $t$ to be:

\begin{equation}
	t = (t_{max}-t_{min})r+t_{min}
\end{equation}

Where $r$ is a random number drawn from a uniform distribution between 0-1. 

Repeating the process of generating random directions $\theta$ and taking random step lengths $t$, will, if enough samples are drawn, generate a uniform sampling of the near optimal feasible set $Z$. Storing all samples $z_i$ in the discrete set $Z^* = {z_i \forall i=0..d}$ where $d$ is the number of samples.\\


"The reason that the Markov chain corresponding to the iterates $Z_0,Z_1,...,Z_d$ converges in distribution to a uniform distribution over $Z$ is easily seen from the fact that 1) it is possible to go from any point in $Z$ to a neighborhood of any other point in one step, and 2 ) the uniform distribution is a stationary distribution of the chain." \cite{Smith1996}\\


The hit and run algorithm is $md$ hard \cite{Belisle1998}. Where m is the number of constraints and d is the number of variables. 

\section{Decrush}

Having sampled $Z$, these sample points must be reverted back to the $F$ domain. This is done with a so called decrush algorithm. 

\begin{equation}
	\mathbf{x}_i = N \mathbf{z}_i + \mathbf{\hat{x}}
\end{equation}

Repeating this for all samples a set containing $d$ samples in $F$ is obtained. 


\section{Application}
	Currently the method has been implemented, with a working presolve and samling method. The model works on Pypsa models and has been run on the Prime server. All code available at :	https://github.com/TimToernes/MAA2 


\section{Issues with this method}

	The hit and run sampler must operate on the entire problem. Meaning that all variables included in the model is part of the problem. The network models considered consist of $\approx 100$ investment variable and $\approx 1e9$	time dependent variables. As it is only the investment variables that is of true interest, the problem becomes extremely large due to the time dependent variables. Currently I have not determined a method of separating the investment and time dependent variables into two separate problems. Methods such as benders-decomposition has been considered. 
	
	As the mixing time (number of samples needed) has complexity $O(n^3)$ for the hit and run algorithm, it is simply not feasible to draw enough samples, as it would require years of computing. Therefore this method has been discarded for now. The method has however been found feasible for problems up to $d=1000$. 

\section{Ideas for further work}

Figure out if sampling $Z$ evenly, generates evenly distributed samples in $F$ when samples are maped from $Z$ to $F$. \\

As of now, i have had sucess with applying the method on problems with $n\approx 10.000$, and i believe that $n\approx 100.000$ is doable on a better pc. This is however one to two order of magnitude from the $1.000.000+$ variables included in most modern energy system optimization models. Therefore i have considered two options. 1) Using decomposition methods, such as Benders decomposition to decompose the problem in to manageable sub-problems. Specifically it is only the non-timedependable variables that are truly of interest, and as these only make up $\approx 1000$ of the variables in the energy system model. By decomposing the timedependable variables from the non-timedependable variables, and sampling the non-timedependable subspace, could be a solution. I have, however, not managed to figure out if this is a viable solution, as decomposition methods often require several iterations between the two sub-problems.  
2) The largest contributor to the large number of variables in energy system models, is the time-series data used. By generating augmented time-series, using techniques such as self organizing maps \cite{Hasan2019}, one can generate shorter time-series, thereby reducing the problem. This is however, a less favorable option than solution (1) as using augmented time-series 

\clearpage
\printbibliography

\end{document}